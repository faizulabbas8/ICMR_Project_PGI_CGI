# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12pevyXxFKY8p72eOfKWmX1QOHXWwjkVZ
"""

from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_extraction(ground_truths, predictions):
    # Assume both are list of dicts with keys like 'doctor', 'patient', etc.
    precision_total, recall_total, f1_total = [], [], []
    keys = ["doctor", "patient", "medicines", "dosage", "instructions"]

    for key in keys:
        y_true, y_pred = [], []
        for gt, pred in zip(ground_truths, predictions):
            gt_val = gt.get(key, [])
            pred_val = pred.get(key, [])
            y_true.extend(gt_val)
            y_pred.extend(pred_val)

        y_true_bin = [1]*len(y_true) + [0]*(len(y_pred)-len(y_true))
        y_pred_bin = [1]*len(y_pred)

        precision_total.append(precision_score(y_true_bin, y_pred_bin, zero_division=0))
        recall_total.append(recall_score(y_true_bin, y_pred_bin, zero_division=0))
        f1_total.append(f1_score(y_true_bin, y_pred_bin, zero_division=0))

    return {
        "precision": sum(precision_total)/len(keys),
        "recall": sum(recall_total)/len(keys),
        "f1": sum(f1_total)/len(keys),
    }